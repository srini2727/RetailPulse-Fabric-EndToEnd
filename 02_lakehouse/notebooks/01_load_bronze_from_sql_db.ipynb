{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22d310",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Notebook: 01_load_bronze_from_sqldb\n",
    "# Purpose : FULL or INCREMENTAL load from Fabric SQL DB -> Lakehouse Bronze Delta\n",
    "# Auth    : Microsoft Entra ID token (no username/password)\n",
    "# Watermark: ops_watermark.last_modified_ts using source modified_ts\n",
    "# =========================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------\n",
    "# Notebook Parameters (Pipeline passes these)\n",
    "# -----------------------------\n",
    "load_mode = mssparkutils.env.getJobParameter(\"load_mode\", \"FULL\")          # FULL | INCR\n",
    "pipeline_name = mssparkutils.env.getJobParameter(\"pipeline_name\", \"rp_orchestrator_dev\")\n",
    "run_date = mssparkutils.env.getJobParameter(\"run_date\", datetime.utcnow().strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# -----------------------------\n",
    "# SQL DB connection\n",
    "# -----------------------------\n",
    "SQL_SERVER = \"g74r6ummslpuxca2gs6zhaj3aa-tux4sdtf5tvu3n6griblkgmll4.database.fabric.microsoft.com\"\n",
    "SQL_DATABASE = \"rp_sqldb_dev-7d103162-1cbc-4e63-baef-4072912d791a\"\n",
    "SQL_SCHEMA = \"dbo\"\n",
    "\n",
    "TABLES = [\n",
    "    \"customers\",\n",
    "    \"orders\",\n",
    "    \"order_items\",\n",
    "    \"payments\",\n",
    "    \"inventory\",\n",
    "    \"returns\"\n",
    "]\n",
    "\n",
    "# Primary keys (match your actual schemas)\n",
    "KEYS = {\n",
    "    \"customers\":   [\"customer_id\"],\n",
    "    \"orders\":      [\"order_id\"],\n",
    "    \"order_items\": [\"order_item_id\"],\n",
    "    \"payments\":    [\"payment_id\"],\n",
    "    \"inventory\":   [\"product_id\"],\n",
    "    \"returns\":     [\"return_id\"],\n",
    "}\n",
    "\n",
    "BRONZE_PREFIX = \"bronze_\"\n",
    "\n",
    "run_id = str(uuid.uuid4())\n",
    "run_ts = datetime.utcnow()\n",
    "\n",
    "print(f\"Mode: {load_mode}\")\n",
    "print(f\"RunId: {run_id}\")\n",
    "print(f\"Pipeline: {pipeline_name}\")\n",
    "print(f\"RunDate: {run_date}\")\n",
    "print(f\"RunTS(UTC): {run_ts}\")\n",
    "\n",
    "# -----------------------------\n",
    "# JDBC + Entra token\n",
    "# -----------------------------\n",
    "jdbc_url = (\n",
    "    f\"jdbc:sqlserver://{SQL_SERVER}:1433;\"\n",
    "    f\"database={SQL_DATABASE};\"\n",
    "    \"encrypt=true;\"\n",
    "    \"trustServerCertificate=false;\"\n",
    "    \"loginTimeout=30;\"\n",
    ")\n",
    "\n",
    "token = mssparkutils.credentials.getToken(\"https://database.windows.net/\")\n",
    "\n",
    "jdbc_props = {\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "    \"accessToken\": token\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# ops_watermark table (must exist from 00_ops_watermark_init)\n",
    "# -----------------------------\n",
    "wm = spark.table(\"ops_watermark\")\n",
    "\n",
    "def get_watermark(table_name: str):\n",
    "    r = (wm.filter(F.col(\"table_name\") == table_name)\n",
    "           .select(\"last_modified_ts\")\n",
    "           .limit(1)\n",
    "           .collect())\n",
    "    return r[0][\"last_modified_ts\"] if r else None\n",
    "\n",
    "def upsert_watermark(table_name: str, new_ts):\n",
    "    if new_ts is None:\n",
    "        return\n",
    "    spark.sql(f\"\"\"\n",
    "      MERGE INTO ops_watermark t\n",
    "      USING (SELECT '{table_name}' AS table_name,\n",
    "                    TIMESTAMP('{new_ts}') AS last_modified_ts,\n",
    "                    current_timestamp() AS updated_at) s\n",
    "      ON t.table_name = s.table_name\n",
    "      WHEN MATCHED THEN UPDATE SET\n",
    "        t.last_modified_ts = s.last_modified_ts,\n",
    "        t.updated_at = s.updated_at\n",
    "      WHEN NOT MATCHED THEN INSERT (table_name,last_modified_ts,updated_at)\n",
    "      VALUES (s.table_name,s.last_modified_ts,s.updated_at)\n",
    "    \"\"\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read helpers\n",
    "# -----------------------------\n",
    "def read_full(table_name: str):\n",
    "    fq = f\"{SQL_SCHEMA}.{table_name}\"\n",
    "    return spark.read.jdbc(url=jdbc_url, table=fq, properties=jdbc_props)\n",
    "\n",
    "def read_incremental(table_name: str, last_ts):\n",
    "    # Use subquery so JDBC pushes filter to SQL engine\n",
    "    last_ts_str = last_ts.strftime(\"%Y-%m-%d %H:%M:%S\") if last_ts else \"1900-01-01 00:00:00\"\n",
    "    q = f\"(SELECT * FROM {SQL_SCHEMA}.{table_name} WHERE modified_ts > '{last_ts_str}') AS src\"\n",
    "    return spark.read.jdbc(url=jdbc_url, table=q, properties=jdbc_props)\n",
    "\n",
    "# -----------------------------\n",
    "# Merge helper\n",
    "# -----------------------------\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "def merge_into_bronze(bronze_table: str, df, key_cols):\n",
    "    if not spark.catalog.tableExists(bronze_table):\n",
    "        (df.write.format(\"delta\")\n",
    "           .mode(\"overwrite\")\n",
    "           .option(\"overwriteSchema\",\"true\")\n",
    "           .saveAsTable(bronze_table))\n",
    "        return \"CREATED\"\n",
    "\n",
    "    tgt = DeltaTable.forName(spark, bronze_table)\n",
    "\n",
    "    cond = \" AND \".join([f\"t.{k} = s.{k}\" for k in key_cols])\n",
    "    (tgt.alias(\"t\")\n",
    "        .merge(df.alias(\"s\"), cond)\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute())\n",
    "\n",
    "    return \"MERGED\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load loop\n",
    "# -----------------------------\n",
    "for t in TABLES:\n",
    "    bronze_table = f\"{BRONZE_PREFIX}{t}\"\n",
    "    key_cols = KEYS[t]\n",
    "\n",
    "    print(f\"\\nüöÄ Loading {t} -> {bronze_table}\")\n",
    "\n",
    "    if load_mode.upper() == \"FULL\":\n",
    "        src = read_full(t)\n",
    "    else:\n",
    "        last_ts = get_watermark(t)\n",
    "        src = read_incremental(t, last_ts)\n",
    "\n",
    "    # If incremental returns no rows, skip\n",
    "    if src.rdd.isEmpty():\n",
    "        print(f\"‚è≠Ô∏è No new rows for {t} (incremental).\")\n",
    "        continue\n",
    "\n",
    "    # Add bronze metadata\n",
    "    out = (src\n",
    "        .withColumn(\"bronze_ingest_ts\", F.lit(run_ts).cast(\"timestamp\"))\n",
    "        .withColumn(\"bronze_run_id\", F.lit(run_id))\n",
    "        .withColumn(\"bronze_source\", F.lit(f\"{SQL_DATABASE}.{SQL_SCHEMA}.{t}\"))\n",
    "    )\n",
    "\n",
    "    # Merge or overwrite\n",
    "    if load_mode.upper() == \"FULL\":\n",
    "        (out.write.format(\"delta\")\n",
    "           .mode(\"overwrite\")\n",
    "           .option(\"overwriteSchema\",\"true\")\n",
    "           .saveAsTable(bronze_table))\n",
    "        action = \"OVERWRITTEN\"\n",
    "    else:\n",
    "        action = merge_into_bronze(bronze_table, out, key_cols)\n",
    "\n",
    "    # Update watermark using max(modified_ts) from this batch\n",
    "    new_wm = out.select(F.max(\"modified_ts\").alias(\"mx\")).collect()[0][\"mx\"]\n",
    "    upsert_watermark(t, new_wm)\n",
    "\n",
    "    cnt = out.count()\n",
    "    print(f\"‚úÖ {bronze_table} {action} | rows processed = {cnt} | new watermark = {new_wm}\")\n",
    "\n",
    "print(\"\\n‚úÖ Bronze load completed.\")\n",
    "display(spark.table(\"ops_watermark\").orderBy(\"table_name\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
