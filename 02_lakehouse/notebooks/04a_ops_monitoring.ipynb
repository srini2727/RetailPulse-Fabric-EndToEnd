{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2718d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# =========================================================\n",
    "# Parameters (later pass from pipeline)\n",
    "# =========================================================\n",
    "PIPELINE_NAME = \"rp_orchestrator_dev\"\n",
    "RUN_DATE = \"2026-01-10\"\n",
    "STAGE = \"OPS_MONITORING\"\n",
    "\n",
    "run_id = str(uuid.uuid4())\n",
    "run_ts = datetime.utcnow()\n",
    "\n",
    "print(f\"RunId: {run_id}\")\n",
    "print(f\"Pipeline: {PIPELINE_NAME}\")\n",
    "print(f\"RunDate: {RUN_DATE}\")\n",
    "print(f\"RunTS(UTC): {run_ts}\")\n",
    "\n",
    "# =========================================================\n",
    "# Helper: table exists\n",
    "# =========================================================\n",
    "def table_exists(tname: str) -> bool:\n",
    "    try:\n",
    "        return spark.catalog.tableExists(tname)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# =========================================================\n",
    "# OPS tables (Lakehouse)\n",
    "# =========================================================\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ops_run_log (\n",
    "  run_id STRING,\n",
    "  pipeline_name STRING,\n",
    "  stage STRING,\n",
    "  run_date STRING,\n",
    "  run_ts TIMESTAMP,\n",
    "  status STRING,\n",
    "  message STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ops_table_metrics (\n",
    "  run_id STRING,\n",
    "  pipeline_name STRING,\n",
    "  run_date STRING,\n",
    "  run_ts TIMESTAMP,\n",
    "  table_name STRING,\n",
    "  row_count LONG,\n",
    "  distinct_key_count LONG,\n",
    "  duplicate_key_count LONG,\n",
    "  null_key_count LONG,\n",
    "  min_ts TIMESTAMP,\n",
    "  max_ts TIMESTAMP,\n",
    "  notes STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# =========================================================\n",
    "# Metrics function\n",
    "# =========================================================\n",
    "def compute_table_metrics(table_name, key_cols=None, ts_candidates=None, notes=\"\"):\n",
    "    if key_cols is None:\n",
    "        key_cols = []\n",
    "    if ts_candidates is None:\n",
    "        ts_candidates = []\n",
    "\n",
    "    if not table_exists(table_name):\n",
    "        return {\n",
    "            \"table_name\": table_name,\n",
    "            \"row_count\": None,\n",
    "            \"distinct_key_count\": None,\n",
    "            \"duplicate_key_count\": None,\n",
    "            \"null_key_count\": None,\n",
    "            \"min_ts\": None,\n",
    "            \"max_ts\": None,\n",
    "            \"notes\": \"SKIPPED (table not found)\"\n",
    "        }\n",
    "\n",
    "    df = spark.table(table_name)\n",
    "    cols = set(df.columns)\n",
    "\n",
    "    row_count = df.count()\n",
    "\n",
    "    # timestamp column detection\n",
    "    ts_col = next((c for c in ts_candidates if c in cols), None)\n",
    "\n",
    "    min_ts = max_ts = None\n",
    "    if ts_col:\n",
    "        stats = df.select(\n",
    "            F.min(ts_col).alias(\"min_ts\"),\n",
    "            F.max(ts_col).alias(\"max_ts\")\n",
    "        ).collect()[0]\n",
    "        min_ts = stats[\"min_ts\"]\n",
    "        max_ts = stats[\"max_ts\"]\n",
    "\n",
    "    usable_keys = [k for k in key_cols if k in cols]\n",
    "\n",
    "    distinct_key_count = duplicate_key_count = null_key_count = None\n",
    "\n",
    "    if usable_keys:\n",
    "        null_cond = None\n",
    "        for k in usable_keys:\n",
    "            c = F.col(k).isNull()\n",
    "            null_cond = c if null_cond is None else null_cond | c\n",
    "\n",
    "        null_key_count = df.filter(null_cond).count()\n",
    "        distinct_key_count = df.select(*usable_keys).distinct().count()\n",
    "        duplicate_key_count = row_count - distinct_key_count\n",
    "    else:\n",
    "        notes = f\"{notes} | Keys not found\"\n",
    "\n",
    "    return {\n",
    "        \"table_name\": table_name,\n",
    "        \"row_count\": row_count,\n",
    "        \"distinct_key_count\": distinct_key_count,\n",
    "        \"duplicate_key_count\": duplicate_key_count,\n",
    "        \"null_key_count\": null_key_count,\n",
    "        \"min_ts\": min_ts,\n",
    "        \"max_ts\": max_ts,\n",
    "        \"notes\": notes.strip()\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# Tables to monitor (ALIGNED TO YOUR SCHEMA)\n",
    "# =========================================================\n",
    "monitor_list = [\n",
    "    # -------- Silver (Lakehouse)\n",
    "    {\"name\": \"silver_products_clean\",        \"keys\": [\"product_id\"],            \"ts\": [\"silver_ingest_ts\"]},\n",
    "    {\"name\": \"silver_orders_clean\",          \"keys\": [\"order_id\"],              \"ts\": [\"order_ts\",\"silver_ingest_ts\"]},\n",
    "    {\"name\": \"silver_order_items_clean\",     \"keys\": [\"order_item_id\"],         \"ts\": [\"silver_ingest_ts\"]},\n",
    "    {\"name\": \"silver_customers_current\",     \"keys\": [\"customer_id\"],           \"ts\": [\"silver_ingest_ts\"]},\n",
    "    {\"name\": \"silver_customers_scd2\",        \"keys\": [\"customer_id\",\"effective_start_ts\"], \"ts\": [\"effective_start_ts\"]},\n",
    "    {\"name\": \"silver_payments_clean\",        \"keys\": [\"payment_id\"],            \"ts\": [\"payment_ts\"]},\n",
    "    {\"name\": \"silver_returns_clean\",         \"keys\": [\"return_id\"],             \"ts\": [\"return_ts\"]},\n",
    "\n",
    "    # -------- Gold staging (Lakehouse)\n",
    "    {\"name\": \"gold_fact_sales_staging\",      \"keys\": [\"order_id\",\"product_id\"], \"ts\": [\"order_ts\"]},\n",
    "    {\"name\": \"gold_fact_payments_staging\",   \"keys\": [\"payment_id\"],            \"ts\": []},\n",
    "    {\"name\": \"gold_fact_returns_staging\",    \"keys\": [\"return_id\"],             \"ts\": []},\n",
    "]\n",
    "\n",
    "# =========================================================\n",
    "# Run START log\n",
    "# =========================================================\n",
    "spark.createDataFrame([(\n",
    "    run_id, PIPELINE_NAME, STAGE, RUN_DATE, run_ts, \"STARTED\", \"OPS monitoring started\"\n",
    ")], [\"run_id\",\"pipeline_name\",\"stage\",\"run_date\",\"run_ts\",\"status\",\"message\"]) \\\n",
    ".write.mode(\"append\").saveAsTable(\"ops_run_log\")\n",
    "\n",
    "# =========================================================\n",
    "# Compute metrics\n",
    "# =========================================================\n",
    "rows = []\n",
    "for item in monitor_list:\n",
    "    m = compute_table_metrics(item[\"name\"], item[\"keys\"], item[\"ts\"])\n",
    "    rows.append((\n",
    "        run_id, PIPELINE_NAME, RUN_DATE, run_ts,\n",
    "        m[\"table_name\"], m[\"row_count\"],\n",
    "        m[\"distinct_key_count\"], m[\"duplicate_key_count\"],\n",
    "        m[\"null_key_count\"], m[\"min_ts\"], m[\"max_ts\"], m[\"notes\"]\n",
    "    ))\n",
    "\n",
    "spark.createDataFrame(rows, [\n",
    "    \"run_id\",\"pipeline_name\",\"run_date\",\"run_ts\",\n",
    "    \"table_name\",\"row_count\",\"distinct_key_count\",\n",
    "    \"duplicate_key_count\",\"null_key_count\",\"min_ts\",\"max_ts\",\"notes\"\n",
    "]).write.mode(\"append\").saveAsTable(\"ops_table_metrics\")\n",
    "\n",
    "# =========================================================\n",
    "# SCD2 current duplicates check (important)\n",
    "# =========================================================\n",
    "if table_exists(\"silver_customers_scd2\"):\n",
    "    dup_cnt = (\n",
    "        spark.table(\"silver_customers_scd2\")\n",
    "        .filter(\"is_current = true\")\n",
    "        .groupBy(\"customer_id\").count()\n",
    "        .filter(\"count > 1\")\n",
    "        .count()\n",
    "    )\n",
    "    print(f\"⚠️ SCD2 current duplicates: {dup_cnt}\")\n",
    "\n",
    "# =========================================================\n",
    "# Run END log\n",
    "# =========================================================\n",
    "spark.createDataFrame([(\n",
    "    run_id, PIPELINE_NAME, STAGE, RUN_DATE, datetime.utcnow(), \"COMPLETED\", \"OPS monitoring completed\"\n",
    ")], [\"run_id\",\"pipeline_name\",\"stage\",\"run_date\",\"run_ts\",\"status\",\"message\"]) \\\n",
    ".write.mode(\"append\").saveAsTable(\"ops_run_log\")\n",
    "\n",
    "print(\"✅ OPS monitoring completed successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
